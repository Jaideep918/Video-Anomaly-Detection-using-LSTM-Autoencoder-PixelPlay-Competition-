{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":126766,"databundleVersionId":15067517,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision import models\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:29.362241Z","iopub.execute_input":"2026-01-08T04:49:29.362666Z","iopub.status.idle":"2026-01-08T04:49:38.480724Z","shell.execute_reply.started":"2026-01-08T04:49:29.362642Z","shell.execute_reply":"2026-01-08T04:49:38.480144Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nBASE_DIR = \"/kaggle/input/pixel-play-26/Avenue_Corrupted-20251221T112159Z-3-001/Avenue_Corrupted/Dataset\"\nTRAIN_DIR = os.path.join(BASE_DIR, \"training_videos\")\nTEST_DIR  = os.path.join(BASE_DIR, \"testing_videos\")\nOUTPUT_PATH = \"/kaggle/working/submission.csv\"\n\n# temporal settings\nFRAME_STRIDE = 1\nTEMPORAL_WINDOW = 2   # frame_t, frame_{t-1}\n\n# training settings (used later)\nBATCH_SIZE = 32\nEPOCHS = 10\nLR = 1e-3\n\n# reproducibility\nSEED = 42\n\nIMG_SIZE = 224\n\n# preprocessing flags\n# USE_GRAYSCALE = True\nUSE_GRAYSCALE = False\nFIX_ORIENTATION = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:38.482457Z","iopub.execute_input":"2026-01-08T04:49:38.482820Z","iopub.status.idle":"2026-01-08T04:49:38.568394Z","shell.execute_reply.started":"2026-01-08T04:49:38.482792Z","shell.execute_reply":"2026-01-08T04:49:38.567424Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1).to(DEVICE)\nIMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1).to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:38.569562Z","iopub.execute_input":"2026-01-08T04:49:38.570089Z","iopub.status.idle":"2026-01-08T04:49:38.792278Z","shell.execute_reply.started":"2026-01-08T04:49:38.570054Z","shell.execute_reply":"2026-01-08T04:49:38.791688Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torchvision.models as models\n\nresnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\nresnet.fc = torch.nn.Identity()   # remove classifier, keep features\nresnet = resnet.to(DEVICE)\nresnet.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:38.793245Z","iopub.execute_input":"2026-01-08T04:49:38.793625Z","iopub.status.idle":"2026-01-08T04:49:39.338919Z","shell.execute_reply.started":"2026-01-08T04:49:38.793595Z","shell.execute_reply":"2026-01-08T04:49:39.338314Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44.7M/44.7M [00:00<00:00, 196MB/s]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Identity()\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class FrameDataset(Dataset):\n    def __init__(self, root_dir, img_size, use_grayscale=True, fix_orientation=True):\n        self.img_size = img_size\n        self.use_grayscale = use_grayscale\n        self.fix_orientation = fix_orientation\n        self.samples = []\n\n        # collect frames: (video_id, frame_path)\n        for video in sorted(os.listdir(root_dir)):\n            video_path = os.path.join(root_dir, video)\n            if not os.path.isdir(video_path):\n                continue\n\n            for fname in sorted(os.listdir(video_path)):\n                if fname.endswith(\".jpg\"):\n                    self.samples.append(\n                        (video, os.path.join(video_path, fname))\n                    )\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _fix_orientation(self, img):\n        gray = np.array(img.convert(\"L\"))\n        top = gray[:50, :].mean()\n        bottom = gray[-50:, :].mean()\n        if top > bottom:\n            img = img.rotate(180)\n        return img\n\n    def __getitem__(self, idx):\n        video_id, path = self.samples[idx]\n        # load image\n        img = Image.open(path)\n\n        # orientation normalization (dataset artifact fix)\n        if self.fix_orientation:\n            img = self._fix_orientation(img)\n\n        # grayscale (safe)\n        if self.use_grayscale:\n            img = img.convert(\"L\")\n\n        # resize\n        img = img.resize((self.img_size, self.img_size))\n\n        # to tensor [0,1]\n        img = torch.from_numpy(np.array(img)).float() / 255.0\n\n        # add channel dimension\n        if self.use_grayscale:\n            img = img.unsqueeze(0)   # (1,H,W)\n        else:\n            img = img.permute(2, 0, 1)  # (C,H,W)\n\n        # video_id: \"01\" → 1\n        vid = int(video_id)\n\n        # filename: \"frame_00939.jpg\" → 939\n        frame_num = int(\n                        os.path.basename(path)\n                        .replace(\".jpg\", \"\")\n                        .split(\"_\")[-1]\n                        )\n\n        full_id = f\"{vid}_{frame_num}\"\n        return img, vid, full_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:39.339819Z","iopub.execute_input":"2026-01-08T04:49:39.340164Z","iopub.status.idle":"2026-01-08T04:49:39.348633Z","shell.execute_reply.started":"2026-01-08T04:49:39.340140Z","shell.execute_reply":"2026-01-08T04:49:39.348022Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset = FrameDataset(\n    root_dir=TEST_DIR,\n    img_size=IMG_SIZE,\n    use_grayscale=USE_GRAYSCALE,\n    fix_orientation=FIX_ORIENTATION\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:39.350859Z","iopub.execute_input":"2026-01-08T04:49:39.351156Z","iopub.status.idle":"2026-01-08T04:49:39.744033Z","shell.execute_reply.started":"2026-01-08T04:49:39.351135Z","shell.execute_reply":"2026-01-08T04:49:39.743285Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = FrameDataset(\n    root_dir=TRAIN_DIR,\n    img_size=IMG_SIZE,\n    use_grayscale=False,        # ResNet expects 3 channels\n    fix_orientation=True\n)\n\nfeatures = []\nvids = []\n\nfor img, vid, _ in tqdm(train_dataset):\n    img = (img.to(DEVICE) - IMAGENET_MEAN) / IMAGENET_STD\n    with torch.no_grad():\n        feat = resnet(img.unsqueeze(0)).squeeze().cpu()\n    features.append(feat)\n    vids.append(vid)\n\nfeatures = torch.stack(features)\nprint(\"Feature shape:\", features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:49:39.744860Z","iopub.execute_input":"2026-01-08T04:49:39.745093Z","iopub.status.idle":"2026-01-08T04:52:25.230475Z","shell.execute_reply.started":"2026-01-08T04:49:39.745072Z","shell.execute_reply":"2026-01-08T04:52:25.229814Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 9204/9204 [02:45<00:00, 55.74it/s]","output_type":"stream"},{"name":"stdout","text":"Feature shape: torch.Size([9204, 512])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"WINDOW = 10\n\nseqs = []\nfor i in range(len(features) - WINDOW):\n    seqs.append(features[i:i+WINDOW])\n\nseqs = torch.stack(seqs)   # (N, T, 512)\nprint(seqs.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:52:25.231338Z","iopub.execute_input":"2026-01-08T04:52:25.231613Z","iopub.status.idle":"2026-01-08T04:52:25.282402Z","shell.execute_reply.started":"2026-01-08T04:52:25.231590Z","shell.execute_reply":"2026-01-08T04:52:25.281655Z"}},"outputs":[{"name":"stdout","text":"torch.Size([9194, 10, 512])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class LSTMAutoEncoder(torch.nn.Module):\n    def __init__(self, feat_dim=512, hidden=256):\n        super().__init__()\n        self.encoder = torch.nn.LSTM(feat_dim, hidden, batch_first=True)\n        self.decoder = torch.nn.LSTM(hidden, feat_dim, batch_first=True)\n\n    def forward(self, x):\n        z, _ = self.encoder(x)\n        out, _ = self.decoder(z)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:52:25.283401Z","iopub.execute_input":"2026-01-08T04:52:25.283924Z","iopub.status.idle":"2026-01-08T04:52:25.288029Z","shell.execute_reply.started":"2026-01-08T04:52:25.283901Z","shell.execute_reply":"2026-01-08T04:52:25.287339Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"ae = LSTMAutoEncoder().to(DEVICE)\nopt = torch.optim.Adam(ae.parameters(), lr=1e-3)\nloss_fn = torch.nn.MSELoss()\n\nX = seqs.to(DEVICE)\n\nfor ep in range(15):\n    opt.zero_grad()\n    out = ae(X)\n    loss = loss_fn(out, X)\n    loss.backward()\n    opt.step()\n    print(f\"Epoch {ep+1} | Loss {loss.item():.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:52:25.288882Z","iopub.execute_input":"2026-01-08T04:52:25.289705Z","iopub.status.idle":"2026-01-08T04:52:31.323573Z","shell.execute_reply.started":"2026-01-08T04:52:25.289671Z","shell.execute_reply":"2026-01-08T04:52:31.322983Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 | Loss 0.913750\nEpoch 2 | Loss 0.839692\nEpoch 3 | Loss 0.741503\nEpoch 4 | Loss 0.624366\nEpoch 5 | Loss 0.527520\nEpoch 6 | Loss 0.451166\nEpoch 7 | Loss 0.392395\nEpoch 8 | Loss 0.349518\nEpoch 9 | Loss 0.320285\nEpoch 10 | Loss 0.300700\nEpoch 11 | Loss 0.287250\nEpoch 12 | Loss 0.277754\nEpoch 13 | Loss 0.271026\nEpoch 14 | Loss 0.266220\nEpoch 15 | Loss 0.262570\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"results = []\nbuffer = []\nprev_vid = None\n\nfor img, vid, fid in dataset:\n\n    img = (img.to(DEVICE) - IMAGENET_MEAN) / IMAGENET_STD\n    with torch.no_grad():\n        feat = resnet(img.unsqueeze(0)).squeeze().cpu()\n\n    if prev_vid != vid:\n        buffer = []\n\n    buffer.append(feat)\n\n    if len(buffer) < WINDOW:\n        score = 0.0\n    else:\n        seq = torch.stack(buffer[-WINDOW:]).unsqueeze(0).to(DEVICE)\n        with torch.no_grad():\n            recon = ae(seq)\n        score = torch.mean((recon.cpu() - seq.cpu())**2).item()\n\n    results.append({\"Id\": fid, \"Predicted\": score})\n    prev_vid = vid\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:52:31.324527Z","iopub.execute_input":"2026-01-08T04:52:31.324790Z","iopub.status.idle":"2026-01-08T04:56:21.640652Z","shell.execute_reply.started":"2026-01-08T04:52:31.324745Z","shell.execute_reply":"2026-01-08T04:56:21.640009Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df = pd.DataFrame(results)\ndf[['vid','frame']] = df['Id'].str.split('_', expand=True).astype(int)\n\ndf['Predicted'] = df.groupby('vid')['Predicted'].transform(\n    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-6)\n)\n\ndf = df[['Id','Predicted']]\ndf.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\nprint(\"Unique IDs:\", df['Id'].nunique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T04:56:21.641736Z","iopub.execute_input":"2026-01-08T04:56:21.642330Z","iopub.status.idle":"2026-01-08T04:56:21.739616Z","shell.execute_reply.started":"2026-01-08T04:56:21.642292Z","shell.execute_reply":"2026-01-08T04:56:21.738938Z"}},"outputs":[{"name":"stdout","text":"Unique IDs: 11706\n","output_type":"stream"}],"execution_count":12}]}